{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VgDWmVan1SO"
   },
   "source": [
    "Start by copying this into your Google Drive!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S4ipRELHrMV"
   },
   "source": [
    "![maastricht-university-logo.jpeg](data:image/jpeg;base64,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)\n",
    "#Faculty of Science and Engineering - Department of Advanced Computer Sciences\n",
    "# Course Advanced Natural Language Processing (ANLP) - Tutorial Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZVnJSqfnp_v"
   },
   "source": [
    "By Jan Scholtes\n",
    "Version 2024-2025.1\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Welcome to the tutorial on Tokenization. In this notebook you will learn how to preprocess text into tokens.\n",
    "\n",
    "This is the basis of any Information Retrieval, Text Mining or NLP process. Tokenization is closely related to sentence detection, stemming, lemmatization and is part of the large NLP research topic named morphology.\n",
    "\n",
    "Tokenization is highly language dependent. In this tutorial we focus on Western-European languages.\n",
    "\n",
    "In this notebook, we will use the Stanford NLTK library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgRVOWBFrjcl"
   },
   "source": [
    "Text extraction and cleanup is an important component of real-world NLP systems. Text extraction allows one to extract text from various electronic file formats (TXT, HTML, XML, PDF, DOCX, XLSX, PPTX, ...) and deals with the encoding of the characters (Unicode, UTF-8, Code pages or ACSII).\n",
    "\n",
    "Libraries such as BeautifulSoup, Scapy or Selenium can assist you with webscraping and parsing text from HTML and XML.\n",
    "\n",
    "You can run the example hereunder to see how a Webpage is scraped and parsed into tags, which can subsequently be questioned (remove the # before this line:\"pprint(soupified.prettify())\" to see the entire HTML file (it is long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEmUPqnWrVx7"
   },
   "outputs": [],
   "source": [
    "# making the necessary imports,\n",
    "! pip install requests\n",
    "! pip install beautifulsoup4\n",
    "\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
    "html = requests.get(myurl, headers={'user-agent': 'Scraper'}).text # query the website\n",
    "soupified = BeautifulSoup(html, 'html.parser') # parse the html in the 'html' variable, and store it in Beautiful Soup format\"\n",
    "\n",
    "#pprint(soupified.prettify())      # for printing the full HTML structure of the webpage\n",
    "\n",
    "question = soupified.find(\"div\", {\"class\": \"question\"}) # find the nevessary tag and class which it belongs to\n",
    "questiontext = question.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
    "print(\"Question: \\n\", questiontext.get_text().strip())\n",
    "answer = soupified.find(\"div\", {\"class\": \"answer\"}) # find the nevessary tag and class which it belongs to\n",
    "answertext = answer.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
    "print(\"Best answer: \\n\", answertext.get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pboMwIeQwxil"
   },
   "source": [
    "PDF can be quite challenging, especially from a formatting point of view. There are also many PDF reverse engineered formats that do not follow the official PDF guideliness completely. For popular formats from Microsoft, Google, Open Office and other vendors, there are several open source libraries to exract text and meta data. For more obscure file types, one has to fall back to commercial solutions such as Oracle Outside In, but these can be expensive.\n",
    "\n",
    "Encoding normalization is important to map various variants of code pages (https://en.wikipedia.org/wiki/Code_page ), ASCII and other encodings to one common Unicode format (https://home.unicode.org/). UTF-8 is the most used one.\n",
    "\n",
    "In this tutorial, we presume all this has been done and we can start with UTF-8 text files that only contain basic line (CR-LF) and tab formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPfiHmSf7dmT"
   },
   "source": [
    "#NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl18k70t1-EM"
   },
   "source": [
    "First we load NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c44v65PO2AeG"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # load tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD3v4P3lOZyE"
   },
   "source": [
    "NLTK also contains many text corpora. Let's import the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk1RSb6VOnDj"
   },
   "outputs": [],
   "source": [
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "movie_reviews.readme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVqQvmAZPDeL"
   },
   "source": [
    "Let's see what is in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POr_DqyYPFKK"
   },
   "outputs": [],
   "source": [
    "raw = movie_reviews.raw()\n",
    "print(raw[0:1000:1]) # print first 1000 chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdCLI7XPPzLi"
   },
   "source": [
    "Let's see if we can detect the long tail that is typical for natural language. First we seperate the text in indivudual words, then we run a frequency analsyis on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-iCAp-ZP3ZZ"
   },
   "outputs": [],
   "source": [
    "corpus = movie_reviews.words()\n",
    "print(corpus)\n",
    "freq_dist = nltk.FreqDist(corpus)\n",
    "print(freq_dist)\n",
    "print(freq_dist.most_common(50))\n",
    "freq_dist.plot(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ID49Xu16QW8z"
   },
   "source": [
    "# Exercise 1: Language Frequencies\n",
    "a. What kind of a relation do you observe? A lineair, polynomial or power relation?  \n",
    "I observe a power=law distribution, which is common in natural language word frequency distributions. This is also known as Zipf's Law, where the most frequent words have very high counts and there is a steep drop-off as you move to less frequent words. \n",
    "b. What are the most frequent words?\n",
    "The most frequent words are 'function words' or stop wprds like \"the,\" \"is,\" \"of,\" \"and,\" etc. \n",
    "c. Do these words have a clear meaning or can they have multiple meaning depending on the context?\n",
    "These are functional words, that means that they don't carry an independent meaning outside of the sentence. Furthermore, these words are either highly present in the corpus and highly present in the single sentences, meaning that they are not very informative about the categorization of the documents. \n",
    "d. Can these words also be used for (long) distance (complex) references?\n",
    "It depends on the word. Function words like (like \"the,\" \"is,\" \"of,\" etc.) have a limit referential power and are not suitable for mantaining context, but we can see that there are some pronouns, and they are able to refer back to or forward to specific entities across sentences or even paragraph, maintaning context over a distance. \n",
    "e. What does this mean for your NLP algorithm?\n",
    "\n",
    "You want to get some meaning out of the sentences by using the NLP algorithm, but this words are not very informative, because they are functional words; because of that, you may need to remove them, in order to help the algorithm to focus on more meaningful words. However, you can sort the words to respect to indexes able to give greater weight to words that appear more frequently within a single document, rather than across the entire corpus; and example is the TF-IDF. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4Tq_8i-Ut3B"
   },
   "source": [
    "ANSWER HERE:\n",
    "\n",
    "1.a\n",
    "I observe a power-law distribution, which is common in natural language word frequency distributions. This is also known as Zipf's Law, where the most frequent words have very high counts and there is a steep drop-off as you move to less frequent words. \n",
    "\n",
    "1.b\n",
    "The most frequent words are 'functional words' or stop wprds like \"the,\" \"is,\" \"of,\" \"and,\" etc. \n",
    "\n",
    "1c.\n",
    "These are functional words, that means that they don't carry an independent meaning outside of the sentence. Furthermore, these words are either highly present in the corpus and highly present in the single sentences, meaning that they are not very informative about the categorization of the documents. \n",
    "\n",
    "1d.\n",
    "It depends on the word. Function words like (like \"the,\" \"is,\" \"of,\" etc.) have a limit referential power and are not suitable for mantaining context, but we can see that there are some pronouns, and they are able to refer back to or forward to specific entities across sentences or even paragraph, maintaning context over a distance. \n",
    "\n",
    "1e.\n",
    "You want to get some meaning out of the sentences by using the NLP algorithm, but this words are not very informative, because they are functional words; because of that, you may need to remove them, in order to help the algorithm to focus on more meaningful words. However, you can sort the words to respect to indexes able to give greater weight to words that appear more frequently within a single document, rather than across the entire corpus; and example is the TF-IDF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApQiVxWzQgqs"
   },
   "source": [
    "As you can also observe, punctuation characters such as .  and , and other ones (: ; \" \" ? ! ) are still in there. This is where tokenization comes in. Tokenization removes punctuations where they are used as sentence and phrase seperation, but leaves them where they are part of a token (e.g. an email address or abbreviation).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AfVoIBc7gh6"
   },
   "source": [
    "#Sentence Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2oTaS-W2KuL"
   },
   "source": [
    "Sentences are the basic components of human language. For all of the applications we discuss in this course (e.g. Machine Translation, Abstracting, ...) the algorithms used to process language presume that the data consists of correct linguistic sentences. When machine learning is applied, this is also done using correct linguistic sentences. Therefor, sentence detection is extremely important in NLP. If language does not have the form of linguistic sentences, then many of the algorithms we teach in this course will not work. This is the case when dealing with tables, graphs or certain types of headers and footers (e.g. an address in the top of a letter).\n",
    "\n",
    "Next, we load the NLTK tokenizer for sentences (sent_tokenize) and for words (word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9gnIhNd2JDD"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "T6CLqSUA29BU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Department of Advanced Computing Sciences - RNA-GB sometimes abbreviated as DACS - \n",
      " is Maastricht University’s largest and oldest department \n",
      " broadly covering the fields of artificial intelligence, data science, computer science, \n",
      " mathematics and robotics. We maintain a large network of public and \n",
      " private partners through our research collaborations and through the \n",
      " award-winning KE@Work programme. In addition, our staff teaches approximately 800 bachelor’s and master’s \n",
      " students in 3 specialized study programmes in Data Science \n",
      " and Artificial Intelligence. The Department of Advanced Computing Sciences \n",
      "  is the new joint identity of the Institute of Data Science (IDS) and the former \n",
      " Department of Data Science and Knowledge Engineering (DKE).\n"
     ]
    }
   ],
   "source": [
    "my_text = \"The Department of Advanced Computing Sciences - RNA-GB sometimes abbreviated as DACS - \\n is Maastricht University’s largest and oldest department \\n broadly covering the fields of artificial intelligence, data science, computer science, \\n mathematics and robotics. We maintain a large network of public and \\n private partners through our research collaborations and through the \\n award-winning KE@Work programme. In addition, our staff teaches approximately 800 bachelor’s and master’s \\n students in 3 specialized study programmes in Data Science \\n and Artificial Intelligence. The Department of Advanced Computing Sciences \\n  is the new joint identity of the Institute of Data Science (IDS) and the former \\n Department of Data Science and Knowledge Engineering (DKE).\"\n",
    "print(my_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA5suVwx7lHD"
   },
   "source": [
    "# Word Detection aka Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "P48pIjx8XW2p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Department of Advanced Computing Sciences - RNA-GB sometimes abbreviated as DACS - \n",
      " is Maastricht University’s largest and oldest department \n",
      " broadly covering the fields of artificial intelligence, data science, computer science, \n",
      " mathematics and robotics.\n",
      "\n",
      "We maintain a large network of public and \n",
      " private partners through our research collaborations and through the \n",
      " award-winning KE@Work programme.\n",
      "\n",
      "In addition, our staff teaches approximately 800 bachelor’s and master’s \n",
      " students in 3 specialized study programmes in Data Science \n",
      " and Artificial Intelligence.\n",
      "\n",
      "The Department of Advanced Computing Sciences \n",
      "  is the new joint identity of the Institute of Data Science (IDS) and the former \n",
      " Department of Data Science and Knowledge Engineering (DKE).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_sentences = sent_tokenize(my_text)\n",
    "# print(my_sentences) # print entire list unformatted\n",
    "print(\"\\n\")\n",
    "for x in range(len(my_sentences)):\n",
    "    print(my_sentences[x]+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqK3BNkLX5X_"
   },
   "source": [
    "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
    "A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ISHpq9JAY4XX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Department of Advanced Computing Sciences - RNA-GB sometimes abbreviated as DACS - \n",
      " is Maastricht University’s largest and oldest department \n",
      " broadly covering the fields of artificial intelligence, data science, computer science, \n",
      " mathematics and robotics.\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    -\n",
      "    RNA-GB\n",
      "    sometimes\n",
      "    abbreviated\n",
      "    as\n",
      "    DACS\n",
      "    -\n",
      "    is\n",
      "    Maastricht\n",
      "    University\n",
      "    ’\n",
      "    s\n",
      "    largest\n",
      "    and\n",
      "    oldest\n",
      "    department\n",
      "    broadly\n",
      "    covering\n",
      "    the\n",
      "    fields\n",
      "    of\n",
      "    artificial\n",
      "    intelligence\n",
      "    ,\n",
      "    data\n",
      "    science\n",
      "    ,\n",
      "    computer\n",
      "    science\n",
      "    ,\n",
      "    mathematics\n",
      "    and\n",
      "    robotics\n",
      "    .\n",
      "Sentence: We maintain a large network of public and \n",
      " private partners through our research collaborations and through the \n",
      " award-winning KE@Work programme.\n",
      "Tokens: \n",
      "    We\n",
      "    maintain\n",
      "    a\n",
      "    large\n",
      "    network\n",
      "    of\n",
      "    public\n",
      "    and\n",
      "    private\n",
      "    partners\n",
      "    through\n",
      "    our\n",
      "    research\n",
      "    collaborations\n",
      "    and\n",
      "    through\n",
      "    the\n",
      "    award-winning\n",
      "    KE\n",
      "    @\n",
      "    Work\n",
      "    programme\n",
      "    .\n",
      "Sentence: In addition, our staff teaches approximately 800 bachelor’s and master’s \n",
      " students in 3 specialized study programmes in Data Science \n",
      " and Artificial Intelligence.\n",
      "Tokens: \n",
      "    In\n",
      "    addition\n",
      "    ,\n",
      "    our\n",
      "    staff\n",
      "    teaches\n",
      "    approximately\n",
      "    800\n",
      "    bachelor\n",
      "    ’\n",
      "    s\n",
      "    and\n",
      "    master\n",
      "    ’\n",
      "    s\n",
      "    students\n",
      "    in\n",
      "    3\n",
      "    specialized\n",
      "    study\n",
      "    programmes\n",
      "    in\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Artificial\n",
      "    Intelligence\n",
      "    .\n",
      "Sentence: The Department of Advanced Computing Sciences \n",
      "  is the new joint identity of the Institute of Data Science (IDS) and the former \n",
      " Department of Data Science and Knowledge Engineering (DKE).\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    is\n",
      "    the\n",
      "    new\n",
      "    joint\n",
      "    identity\n",
      "    of\n",
      "    the\n",
      "    Institute\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    (\n",
      "    IDS\n",
      "    )\n",
      "    and\n",
      "    the\n",
      "    former\n",
      "    Department\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Knowledge\n",
      "    Engineering\n",
      "    (\n",
      "    DKE\n",
      "    )\n",
      "    .\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    print(\"Sentence: \"+str(sentence))\n",
    "    my_words = word_tokenize(sentence)\n",
    "    print(\"Tokens: \")\n",
    "    for x in range(len(my_words)):\n",
    "      print(\"    \"+str(my_words[x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnvfN0OsairB"
   },
   "source": [
    "As you can observe, there are still punctuation in the list of tokens. In NLTK these can be removed by using a regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "uERolN74aqXt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Department of Advanced Computing Sciences - RNA-GB sometimes abbreviated as DACS - \n",
      " is Maastricht University’s largest and oldest department \n",
      " broadly covering the fields of artificial intelligence, data science, computer science, \n",
      " mathematics and robotics.\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    RNA\n",
      "    GB\n",
      "    sometimes\n",
      "    abbreviated\n",
      "    as\n",
      "    DACS\n",
      "    is\n",
      "    Maastricht\n",
      "    University\n",
      "    s\n",
      "    largest\n",
      "    and\n",
      "    oldest\n",
      "    department\n",
      "    broadly\n",
      "    covering\n",
      "    the\n",
      "    fields\n",
      "    of\n",
      "    artificial\n",
      "    intelligence\n",
      "    data\n",
      "    science\n",
      "    computer\n",
      "    science\n",
      "    mathematics\n",
      "    and\n",
      "    robotics\n",
      "Sentence: We maintain a large network of public and \n",
      " private partners through our research collaborations and through the \n",
      " award-winning KE@Work programme.\n",
      "Tokens: \n",
      "    We\n",
      "    maintain\n",
      "    a\n",
      "    large\n",
      "    network\n",
      "    of\n",
      "    public\n",
      "    and\n",
      "    private\n",
      "    partners\n",
      "    through\n",
      "    our\n",
      "    research\n",
      "    collaborations\n",
      "    and\n",
      "    through\n",
      "    the\n",
      "    award\n",
      "    winning\n",
      "    KE\n",
      "    Work\n",
      "    programme\n",
      "Sentence: In addition, our staff teaches approximately 800 bachelor’s and master’s \n",
      " students in 3 specialized study programmes in Data Science \n",
      " and Artificial Intelligence.\n",
      "Tokens: \n",
      "    In\n",
      "    addition\n",
      "    our\n",
      "    staff\n",
      "    teaches\n",
      "    approximately\n",
      "    800\n",
      "    bachelor\n",
      "    s\n",
      "    and\n",
      "    master\n",
      "    s\n",
      "    students\n",
      "    in\n",
      "    3\n",
      "    specialized\n",
      "    study\n",
      "    programmes\n",
      "    in\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Artificial\n",
      "    Intelligence\n",
      "Sentence: The Department of Advanced Computing Sciences \n",
      "  is the new joint identity of the Institute of Data Science (IDS) and the former \n",
      " Department of Data Science and Knowledge Engineering (DKE).\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    is\n",
      "    the\n",
      "    new\n",
      "    joint\n",
      "    identity\n",
      "    of\n",
      "    the\n",
      "    Institute\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    IDS\n",
      "    and\n",
      "    the\n",
      "    former\n",
      "    Department\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Knowledge\n",
      "    Engineering\n",
      "    DKE\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "new_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for sentence in my_sentences:\n",
    "    print(\"Sentence: \"+str(sentence))\n",
    "    my_words = new_tokenizer.tokenize(sentence)\n",
    "    print(\"Tokens: \")\n",
    "    for x in range(len(my_words)):\n",
    "      print(\"    \"+str(my_words[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhgIu4uKcbmv"
   },
   "source": [
    "#Exercise 2: Preprocessing for Tokenization\n",
    "As you can observe, our tokenizer cannot deal with tokens such as \"Bachelor's\" or \"Master's\". This has to do with the fact that the NLTK tokenizer does not recognize 's as 'its'. It is not part of the tokenizer's code to deal with that. When dealing with chemical formulas, RNA/DNA, part numbers, phone numbers (with spaces, -, . etc) we see similar problems. When there are problems with the tokenization, the errors propagate in subsequent parts of the processing.\n",
    "\n",
    "We can address this problem by applying preprocessing and normalization of such tokens.\n",
    "\n",
    "Write some code to preprocess 's into its and then do proper tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL39SsVJc3QQ"
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Department of Advanced Computing Sciences - RNA-GB sometimes abbreviated as DACS - \n",
      " is Maastricht University’s largest and oldest department \n",
      " broadly covering the fields of artificial intelligence, data science, computer science, \n",
      " mathematics and robotics.\n",
      "Normalized sentence: The Department of Advanced Computing Sciences - RNA-GB sometimes abbreviated as DACS - \n",
      " is Maastricht University its largest and oldest department \n",
      " broadly covering the fields of artificial intelligence, data science, computer science, \n",
      " mathematics and robotics.\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    -\n",
      "    RNA-GB\n",
      "    sometimes\n",
      "    abbreviated\n",
      "    as\n",
      "    DACS\n",
      "    -\n",
      "    is\n",
      "    Maastricht\n",
      "    University\n",
      "    its\n",
      "    largest\n",
      "    and\n",
      "    oldest\n",
      "    department\n",
      "    broadly\n",
      "    covering\n",
      "    the\n",
      "    fields\n",
      "    of\n",
      "    artificial\n",
      "    intelligence\n",
      "    ,\n",
      "    data\n",
      "    science\n",
      "    ,\n",
      "    computer\n",
      "    science\n",
      "    ,\n",
      "    mathematics\n",
      "    and\n",
      "    robotics\n",
      "    .\n",
      "Sentence: We maintain a large network of public and \n",
      " private partners through our research collaborations and through the \n",
      " award-winning KE@Work programme.\n",
      "Normalized sentence: We maintain a large network of public and \n",
      " private partners through our research collaborations and through the \n",
      " award-winning KE@Work programme.\n",
      "Tokens: \n",
      "    We\n",
      "    maintain\n",
      "    a\n",
      "    large\n",
      "    network\n",
      "    of\n",
      "    public\n",
      "    and\n",
      "    private\n",
      "    partners\n",
      "    through\n",
      "    our\n",
      "    research\n",
      "    collaborations\n",
      "    and\n",
      "    through\n",
      "    the\n",
      "    award-winning\n",
      "    KE\n",
      "    @\n",
      "    Work\n",
      "    programme\n",
      "    .\n",
      "Sentence: In addition, our staff teaches approximately 800 bachelor’s and master’s \n",
      " students in 3 specialized study programmes in Data Science \n",
      " and Artificial Intelligence.\n",
      "Normalized sentence: In addition, our staff teaches approximately 800 bachelor its and master its \n",
      " students in 3 specialized study programmes in Data Science \n",
      " and Artificial Intelligence.\n",
      "Tokens: \n",
      "    In\n",
      "    addition\n",
      "    ,\n",
      "    our\n",
      "    staff\n",
      "    teaches\n",
      "    approximately\n",
      "    800\n",
      "    bachelor\n",
      "    its\n",
      "    and\n",
      "    master\n",
      "    its\n",
      "    students\n",
      "    in\n",
      "    3\n",
      "    specialized\n",
      "    study\n",
      "    programmes\n",
      "    in\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Artificial\n",
      "    Intelligence\n",
      "    .\n",
      "Sentence: The Department of Advanced Computing Sciences \n",
      "  is the new joint identity of the Institute of Data Science (IDS) and the former \n",
      " Department of Data Science and Knowledge Engineering (DKE).\n",
      "Normalized sentence: The Department of Advanced Computing Sciences \n",
      "  is the new joint identity of the Institute of Data Science (IDS) and the former \n",
      " Department of Data Science and Knowledge Engineering (DKE).\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    is\n",
      "    the\n",
      "    new\n",
      "    joint\n",
      "    identity\n",
      "    of\n",
      "    the\n",
      "    Institute\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    (\n",
      "    IDS\n",
      "    )\n",
      "    and\n",
      "    the\n",
      "    former\n",
      "    Department\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Knowledge\n",
      "    Engineering\n",
      "    (\n",
      "    DKE\n",
      "    )\n",
      "    .\n",
      "Sentence: ['The', 'Department', 'of', 'Advanced', 'Computing', 'Sciences', '-', 'RNA-GB', 'sometimes', 'abbreviated', 'as', 'DACS', '-', 'is', 'Maastricht', 'University', 'its', 'largest', 'and', 'oldest', 'department', 'broadly', 'covering', 'the', 'fields', 'of', 'artificial', 'intelligence', ',', 'data', 'science', ',', 'computer', 'science', ',', 'mathematics', 'and', 'robotics', '.']\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    sometimes\n",
      "    abbreviated\n",
      "    as\n",
      "    DACS\n",
      "    is\n",
      "    Maastricht\n",
      "    University\n",
      "    its\n",
      "    largest\n",
      "    and\n",
      "    oldest\n",
      "    department\n",
      "    broadly\n",
      "    covering\n",
      "    the\n",
      "    fields\n",
      "    of\n",
      "    artificial\n",
      "    intelligence\n",
      "    data\n",
      "    science\n",
      "    computer\n",
      "    science\n",
      "    mathematics\n",
      "    and\n",
      "    robotics\n",
      "Sentence: ['We', 'maintain', 'a', 'large', 'network', 'of', 'public', 'and', 'private', 'partners', 'through', 'our', 'research', 'collaborations', 'and', 'through', 'the', 'award-winning', 'KE', '@', 'Work', 'programme', '.']\n",
      "Tokens: \n",
      "    We\n",
      "    maintain\n",
      "    a\n",
      "    large\n",
      "    network\n",
      "    of\n",
      "    public\n",
      "    and\n",
      "    private\n",
      "    partners\n",
      "    through\n",
      "    our\n",
      "    research\n",
      "    collaborations\n",
      "    and\n",
      "    through\n",
      "    the\n",
      "    KE\n",
      "    Work\n",
      "    programme\n",
      "Sentence: ['In', 'addition', ',', 'our', 'staff', 'teaches', 'approximately', '800', 'bachelor', 'its', 'and', 'master', 'its', 'students', 'in', '3', 'specialized', 'study', 'programmes', 'in', 'Data', 'Science', 'and', 'Artificial', 'Intelligence', '.']\n",
      "Tokens: \n",
      "    In\n",
      "    addition\n",
      "    our\n",
      "    staff\n",
      "    teaches\n",
      "    approximately\n",
      "    bachelor\n",
      "    its\n",
      "    and\n",
      "    master\n",
      "    its\n",
      "    students\n",
      "    in\n",
      "    specialized\n",
      "    study\n",
      "    programmes\n",
      "    in\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Artificial\n",
      "    Intelligence\n",
      "Sentence: ['The', 'Department', 'of', 'Advanced', 'Computing', 'Sciences', 'is', 'the', 'new', 'joint', 'identity', 'of', 'the', 'Institute', 'of', 'Data', 'Science', '(', 'IDS', ')', 'and', 'the', 'former', 'Department', 'of', 'Data', 'Science', 'and', 'Knowledge', 'Engineering', '(', 'DKE', ')', '.']\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    is\n",
      "    the\n",
      "    new\n",
      "    joint\n",
      "    identity\n",
      "    of\n",
      "    the\n",
      "    Institute\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    IDS\n",
      "    and\n",
      "    the\n",
      "    former\n",
      "    Department\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Knowledge\n",
      "    Engineering\n",
      "    DKE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aurora Pia\n",
      "[nltk_data]     Ghiardell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Contractions expansion\n",
    "import re\n",
    "\n",
    "contractions_dict = {\n",
    "    \"ain’t\": \"are not\", \"’s\": \" its\", \"aren’t\": \"are not\", \"can’t\": \"cannot\", \n",
    "    \"can’t’ve\": \"cannot have\", \"’cause\": \" because\", \"could’ve\": \"could have\", \n",
    "    \"couldn’t\": \"could not\", \"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \n",
    "    \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \n",
    "    \"hadn’t’ve\": \"had not have\", \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \n",
    "    \"he’d\": \" he would\", \"he’d’ve\": \" he would have\", \"he’ll\": \" he will\", \n",
    "    \"he’ll’ve\": \" he will have\", \"how’d\": \" how did\", \"how’d’y\": \" how do you\", \n",
    "    \"how’ll\": \" how will\", \"I’d\": \" I would\", \"I’d’ve\": \" I would have\", \n",
    "    \"I’ll\": \" I will\", \"I’ll’ve\": \" I will have\", \"I’m\": \" I am\", \"I’ve\": \" I have\", \n",
    "    \"isn’t\": \"is not\", \"it’d\": \" it would\", \"it’d’ve\": \" it would have\", \n",
    "    \"it’ll\": \" it will\", \"it’ll’ve\": \" it will have\", \"let’s\": \" let us\", \n",
    "    \"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \n",
    "    \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \n",
    "    \"mustn’t\": \"must not\", \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \n",
    "    \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \n",
    "    \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \n",
    "    \"shan’t’ve\": \"shall not have\", \"she’d\": \" she would\", \"she’d’ve\": \" she would have\", \n",
    "    \"she’ll\": \" she will\", \"she’ll’ve\": \" she will have\", \"should’ve\": \"should have\", \n",
    "    \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\", \n",
    "    \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"there’d\": \"there would\", \n",
    "    \"there’d’ve\": \"there would have\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \n",
    "    \"they’ll\": \"they will\", \"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \n",
    "    \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \n",
    "    \"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \n",
    "    \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\", \n",
    "    \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\", \"what’ve\": \"what have\", \n",
    "    \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’ve\": \"where have\", \n",
    "    \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\", \"who’ve\": \"who have\", \n",
    "    \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \n",
    "    \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \n",
    "    \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \n",
    "    \"y’all’d’ve\": \"you all would have\", \"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \n",
    "    \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \n",
    "    \"you’ll’ve\": \"you will have\", \"you’re\": \"you are\", \"you’ve\": \"you have\"\n",
    "}\n",
    "\n",
    "contractions_re = re.compile('|'.join(re.escape(key) for key in contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = re.findall(r'\\b\\w+(?:-\\w+)*\\'?\\w*|\\b[A-Z][a-z]?\\d*-?[A-Z]?\\d*|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "\n",
    "tok_sentences = []\n",
    "for sentence in my_sentences:\n",
    "    print(\"Sentence: \" + str(sentence))\n",
    "    sentence = expand_contractions(sentence)\n",
    "    print(\"Normalized sentence: \" + str(sentence))\n",
    "    my_words = tokenize_text(sentence)\n",
    "    tok_sentences.append(my_words)\n",
    "    print(\"Tokens: \")\n",
    "    for x in range(len(my_words)):\n",
    "      print(\"    \"+str(my_words[x]))\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['The', 'Department', 'of', 'Advanced', 'Computing', 'Sciences', '-', 'RNA-GB', 'sometimes', 'abbreviated', 'as', 'DACS', '-', 'is', 'Maastricht', 'University', 'its', 'largest', 'and', 'oldest', 'department', 'broadly', 'covering', 'the', 'fields', 'of', 'artificial', 'intelligence', ',', 'data', 'science', ',', 'computer', 'science', ',', 'mathematics', 'and', 'robotics', '.']\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    sometimes\n",
      "    abbreviated\n",
      "    as\n",
      "    DACS\n",
      "    is\n",
      "    Maastricht\n",
      "    University\n",
      "    its\n",
      "    largest\n",
      "    and\n",
      "    oldest\n",
      "    department\n",
      "    broadly\n",
      "    covering\n",
      "    the\n",
      "    fields\n",
      "    of\n",
      "    artificial\n",
      "    intelligence\n",
      "    data\n",
      "    science\n",
      "    computer\n",
      "    science\n",
      "    mathematics\n",
      "    and\n",
      "    robotics\n",
      "Sentence: ['We', 'maintain', 'a', 'large', 'network', 'of', 'public', 'and', 'private', 'partners', 'through', 'our', 'research', 'collaborations', 'and', 'through', 'the', 'award-winning', 'KE', '@', 'Work', 'programme', '.']\n",
      "Tokens: \n",
      "    We\n",
      "    maintain\n",
      "    a\n",
      "    large\n",
      "    network\n",
      "    of\n",
      "    public\n",
      "    and\n",
      "    private\n",
      "    partners\n",
      "    through\n",
      "    our\n",
      "    research\n",
      "    collaborations\n",
      "    and\n",
      "    through\n",
      "    the\n",
      "    KE\n",
      "    Work\n",
      "    programme\n",
      "Sentence: ['In', 'addition', ',', 'our', 'staff', 'teaches', 'approximately', '800', 'bachelor', 'its', 'and', 'master', 'its', 'students', 'in', '3', 'specialized', 'study', 'programmes', 'in', 'Data', 'Science', 'and', 'Artificial', 'Intelligence', '.']\n",
      "Tokens: \n",
      "    In\n",
      "    addition\n",
      "    our\n",
      "    staff\n",
      "    teaches\n",
      "    approximately\n",
      "    bachelor\n",
      "    its\n",
      "    and\n",
      "    master\n",
      "    its\n",
      "    students\n",
      "    in\n",
      "    specialized\n",
      "    study\n",
      "    programmes\n",
      "    in\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Artificial\n",
      "    Intelligence\n",
      "Sentence: ['The', 'Department', 'of', 'Advanced', 'Computing', 'Sciences', 'is', 'the', 'new', 'joint', 'identity', 'of', 'the', 'Institute', 'of', 'Data', 'Science', '(', 'IDS', ')', 'and', 'the', 'former', 'Department', 'of', 'Data', 'Science', 'and', 'Knowledge', 'Engineering', '(', 'DKE', ')', '.']\n",
      "Tokens: \n",
      "    The\n",
      "    Department\n",
      "    of\n",
      "    Advanced\n",
      "    Computing\n",
      "    Sciences\n",
      "    is\n",
      "    the\n",
      "    new\n",
      "    joint\n",
      "    identity\n",
      "    of\n",
      "    the\n",
      "    Institute\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    IDS\n",
      "    and\n",
      "    the\n",
      "    former\n",
      "    Department\n",
      "    of\n",
      "    Data\n",
      "    Science\n",
      "    and\n",
      "    Knowledge\n",
      "    Engineering\n",
      "    DKE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aurora Pia\n",
      "[nltk_data]     Ghiardell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove Punctuation\n",
    "nltk.download('punkt')\n",
    "def remove_punct(token):\n",
    "    return [word for word in token if word.isalpha()]\n",
    "\n",
    "for sentence in tok_sentences:\n",
    "    print(\"Sentence: \" + str(sentence))\n",
    "    my_words = remove_punct(sentence)\n",
    "    print(\"Tokens: \")\n",
    "    for x in range(len(my_words)):\n",
    "      print(\"    \"+str(my_words[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-OCi_nxIPYP"
   },
   "source": [
    "# Generate a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STaUFJPJJIrl"
   },
   "source": [
    "A vocabulary is a data structure containing every unique word used in the corpus only once and in alphabetical order. This can be used as a dictionairy in NLP or as the basis of a search index in information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "guS6cWr6ISTU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '800', 'a', 'abbreviated', 'addition', 'advanced', 'and', 'approximately', 'artificial', 'as', 'award', 'bachelor', 'broadly', 'collaborations', 'computer', 'computing', 'covering', 'dacs', 'data', 'department', 'dke', 'engineering', 'fields', 'former', 'gb', 'identity', 'ids', 'in', 'institute', 'intelligence', 'is', 'joint', 'ke', 'knowledge', 'large', 'largest', 'maastricht', 'maintain', 'master', 'mathematics', 'network', 'new', 'of', 'oldest', 'our', 'partners', 'private', 'programme', 'programmes', 'public', 'research', 'rna', 'robotics', 's', 'science', 'sciences', 'sometimes', 'specialized', 'staff', 'students', 'study', 'teaches', 'the', 'through', 'university', 'we', 'winning', 'work']\n",
      "Tokens: 109\n",
      "Vocabulary: 68\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = new_tokenizer.tokenize(my_text.lower()) #use the tokenizer that removes punctuation\n",
    "vocab = sorted(set(corpus_tokens))\n",
    "print(vocab)\n",
    "print(\"Tokens:\", len(corpus_tokens))\n",
    "print(\"Vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51E04B3g7pz6"
   },
   "source": [
    "#Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxnKcmMt4tDr"
   },
   "source": [
    "In the past, when computer resources were still limited, highly frequent words were often removed in information retrieval applications. These are named stop-words or noise-words. These are words such as \"the, on, in, a, be, or, and, an, for, to, ...\". If such a word is removed, one can no longer search for them. Imagine searching for \"to be or not to be\", which is no longer after noise word removal.\n",
    "\n",
    "In text-mining and advanced NLP (especially when using context-sensitive deep-leaning models), these words often contain important clues on the meaning of language. Removing them will completely destroy the performance of the models.\n",
    "\n",
    "So, these days as computer resources are much larger and context sensitive models need these words for disambiguation, noise words are no longer removed.\n",
    "\n",
    "But let's try how to remove them using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "95Xp0Zlc5qOM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords from NLTK: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "Tokenized corpus: ['the', 'department', 'of', 'advanced', 'computing', 'sciences', 'rna', 'gb', 'sometimes', 'abbreviated', 'as', 'dacs', 'is', 'maastricht', 'university', 's', 'largest', 'and', 'oldest', 'department', 'broadly', 'covering', 'the', 'fields', 'of', 'artificial', 'intelligence', 'data', 'science', 'computer', 'science', 'mathematics', 'and', 'robotics', 'we', 'maintain', 'a', 'large', 'network', 'of', 'public', 'and', 'private', 'partners', 'through', 'our', 'research', 'collaborations', 'and', 'through', 'the', 'award', 'winning', 'ke', 'work', 'programme', 'in', 'addition', 'our', 'staff', 'teaches', 'approximately', '800', 'bachelor', 's', 'and', 'master', 's', 'students', 'in', '3', 'specialized', 'study', 'programmes', 'in', 'data', 'science', 'and', 'artificial', 'intelligence', 'the', 'department', 'of', 'advanced', 'computing', 'sciences', 'is', 'the', 'new', 'joint', 'identity', 'of', 'the', 'institute', 'of', 'data', 'science', 'ids', 'and', 'the', 'former', 'department', 'of', 'data', 'science', 'and', 'knowledge', 'engineering', 'dke']\n",
      "Tokenized corpus without stopwords: ['department', 'advanced', 'computing', 'sciences', 'rna', 'gb', 'sometimes', 'abbreviated', 'dacs', 'maastricht', 'university', 'largest', 'oldest', 'department', 'broadly', 'covering', 'fields', 'artificial', 'intelligence', 'data', 'science', 'computer', 'science', 'mathematics', 'robotics', 'maintain', 'large', 'network', 'public', 'private', 'partners', 'research', 'collaborations', 'award', 'winning', 'ke', 'work', 'programme', 'addition', 'staff', 'teaches', 'approximately', '800', 'bachelor', 'master', 'students', '3', 'specialized', 'study', 'programmes', 'data', 'science', 'artificial', 'intelligence', 'department', 'advanced', 'computing', 'sciences', 'new', 'joint', 'identity', 'institute', 'data', 'science', 'ids', 'former', 'department', 'data', 'science', 'knowledge', 'engineering', 'dke']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aurora Pia\n",
      "[nltk_data]     Ghiardell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(\"Stopwords from NLTK:\", stopwords.words('english'))\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "# we use the token list without punctuations\n",
    "print(\"Tokenized corpus:\",corpus_tokens)\n",
    "#now remove stopwords\n",
    "tokenized_corpus_without_stopwords = [i for i in corpus_tokens if not i in stop_words_nltk]\n",
    "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7bgF2uT7P0E"
   },
   "source": [
    "#Exercise 3: Stopwords and Case Sensitivity\n",
    "\n",
    "ANSWER HERE\n",
    "\n",
    "3.a What do you observe with respect to case sensitivity of proper names and words such as \"I\".\n",
    "\n",
    "The tokenized output reveals that proper names, such as \"Maastricht University\" and \"Artificial Intelligence,\" are transformed to lowercase, appearing as \"maastricht\" and \"university.\" This alteration can lead to a significant loss of contextual and identity information. For instance, \"Maastricht\" specifically refers to a city, while \"maastricht\" lacks the same distinct meaning. Similarly, the phrase \"Artificial Intelligence\" loses its significance when split into lowercase words, obscuring its identity as a recognized field of study.\n",
    "Additionally, the representation of the word \"I\" as \"i\" poses its own challenges. The pronoun \"I\" denotes a specific individual, conveying personal agency and intention. In contrast, using \"i\" may imply a generic action rather than referring to a particular person. This loss of capitalization can dilute the personal significance and clarity of the text, leading to potential misunderstandings in communication.\n",
    "\n",
    "3.b How can we solve that?'\n",
    "\n",
    "You can use a predefined list of proper nouns or leveraging Named Entity Recognition (NER) techniques.\n",
    "\n",
    "3.c Could this actions also lead to unwanted side effects?\n",
    "\n",
    "Yes, these actions can lead to unwanted side effects. By converting everything to lowercase and splitting words, we introduce ambiguity into the text. As mentioned earlier, the pronoun \"I\" is a prime example of this issue; when represented as \"i,\" it loses its significance as a specific reference to an individual, potentially leading to misunderstandings.\n",
    "Furthermore, the separation of terms like \"Artificial Intelligence\" means that we are modeling these words individually, which can obscure their relationship and meaning when used together. In a diverse corpus, this could hinder the ability to connect these words effectively, resulting in a lack of clarity regarding their combined significance.\n",
    "Similarly, in the case of \"DACS,\" which stands for \"Department of Advanced Computing Sciences,\" splitting it into \"department,\" \"advanced,\" \"computing,\" and \"sciences\" loses the cohesiveness of the original acronym. This disaggregation can lead to confusion, as the individual terms may not convey the intended meaning without the context provided by their original form. Thus, the process of lowering and splitting words compromises the clarity and richness of the text, ultimately affecting its interpretation and analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsjAz4lF7Y0K"
   },
   "source": [
    "#Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UYjccj-RvK8"
   },
   "source": [
    "Stemming is the process of removing suffixes and reducing the word to some base form such that all different variations of a word can be represented by one form. Stemming uses rules and may not always result in the correct linguistic base form. However, it is fast and therefor often used by search engines. As we discussed in the lecture, a well-known stemmer for the English language is the Porter stemmer.\n",
    "\n",
    "Let's try it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-o3TscOrSXGT"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer= PorterStemmer()\n",
    "print(\"before stemming -> after stemming\")\n",
    "for word in corpus_tokens:\n",
    "  print(str(word) + \" -> \" + str(stemmer.stem(word)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XnibDb_Xs1y"
   },
   "source": [
    "As you can see, \"students\" is converted into \"student\", but \"Science\" is converted into \"scien\". There are other non-linguistically correct transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDc2SVqq7w2a"
   },
   "source": [
    "#Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kav2cXbpYKa9"
   },
   "source": [
    "This why we prefer to use lemmatization for linguistic applications other than search engines. Lemmatization is the process of mapping all tokens to its base-linguistic form: the \"lemma\". So \"better\" should be converted to \"good\" and \"is\" to \"be\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv5796gKY8C7"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # downloading wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "print(\"before lemmatization -> after lemmarization\")\n",
    "for word in corpus_tokens:\n",
    "  print(str(word) + \" -> \" + str(lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4l3z9ajbeIs"
   },
   "source": [
    "As you can observe, only plurals and other basic operations are performed. But \"is\" not converted to \"be\". Neither are several verb inflections. This is because Lemmatization requires more linguistic knowledge: it need to know whether we are dealing with, for instance, a verb, noun or a adjectice. We call these gramatical roles \"part-of-speech\" or POS tags. These will be discussed in the next lecture: Syntax and Semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9Cq2Xx2b4I6"
   },
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('better'))\n",
    "print(lemmatizer.lemmatize('better',pos='a')) # a for Adjective\n",
    "print(lemmatizer.lemmatize('is'))\n",
    "print(lemmatizer.lemmatize('is',pos='v'))  # v for Verb\n",
    "print(lemmatizer.lemmatize('is',pos='a'))\n",
    "print(lemmatizer.lemmatize('is',pos='n'))  # n for Noun\n",
    "print(lemmatizer.lemmatize('richer',pos='n'))\n",
    "print(lemmatizer.lemmatize('richer',pos='a'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QE6ntyME72S0"
   },
   "source": [
    "# Text-Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyicUNL2enhr"
   },
   "source": [
    "In social media, one can run in short-cuts, slang, hash-tags, or emoticons. These can be concerted to their textual forms. Phone numbers, dates and monetary amounts can be written in many different forms. Sometimes, one can even decide to convert all text to either lower case or upper case. This may cause problems in some applications and should be used carefully. We will discuss this in more detail in the course Text Mining, where this is more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeGSKRnx78Ry"
   },
   "source": [
    "#Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhONHqV2fiU6"
   },
   "source": [
    "Almost all NLP models and algorithms are very language specific: this means that one can only use them with the intenred language. Using them on other language will result in random behavior.  \n",
    "\n",
    "So, language detection (often per sentence or minimally per paragrpah) is essential for any type of NLP application to perform correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njzaEIDBgFAZ"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect\n",
    "from langdetect import detect, detect_langs\n",
    "def language_detection(text, method = \"single\"):\n",
    "  if(method.lower() != \"single\"):\n",
    "    result = detect_langs(text)\n",
    "  else:\n",
    "    result = detect(text)\n",
    "  return result\n",
    "\n",
    "multilingual_text = \"Elle est vraiment éfficace dans la détection de langue.\"\n",
    "print(language_detection(multilingual_text))\n",
    "multilingual_text = \"Het is enorm makkelijk om een taal te herkennen!\"\n",
    "print(language_detection(multilingual_text))\n",
    "multilingual_text = \"Es ist wirklich effektiv bei der Spracherkennung.\"\n",
    "print(language_detection(multilingual_text))\n",
    "multilingual_text = \"Nó thực sự hiệu quả trong việc phát hiện ngôn ngữ.\"\n",
    "print(language_detection(multilingual_text))\n",
    "multilingual_text = \"إنه فعال حقًا في اكتشاف اللغة.\"\n",
    "print(language_detection(multilingual_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmCN9E977_mi"
   },
   "source": [
    "# Transliteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "munkN882hiya"
   },
   "source": [
    "Transliteration refers to the method of mapping from one system of writing to another based on phonetic similarity. With this tool, you type in Latin letters (e.g. a, b, c etc.), which are converted to characters that have similar pronunciation in the target language. For transliteration, you need to select the target language. So, results for a transliteration of a Arabic name into English, French or German can be very different for similar names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ciweSsXiu0B"
   },
   "source": [
    "Лев Николаевич Толстой\n",
    "\n",
    "results in different forms of transliteration for different target languages:\n",
    "\n",
    "Lev Nikolayevich Tolstoy\n",
    "\n",
    "Léon Tolstoï\n",
    "\n",
    "Lev Tolstoj\n",
    "\n",
    "León Tolstó\n",
    "\n",
    "Lev Tolstoy\n",
    "\n",
    "Lav Tolstoj\n",
    "\n",
    "Lev Tolsto\n",
    "\n",
    "Liuni Tolstoi\n",
    "\n",
    "Ļevs Tolstojs\n",
    "\n",
    "Levs Tuolstuos\n",
    "\n",
    "...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvS5d3LEiNcp"
   },
   "source": [
    "A Python library for transliteration can be found here: https://pypi.org/project/transliterate/. We will discuss this in more detail in the lecture on Machine Translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV7f89o0VYzo"
   },
   "source": [
    "#Exercise 4: Final reflections on tokenization.\n",
    "\n",
    "As we have seen in this tutorial, tokenization is a very important component of NLP. Any error in this phase will propagate in the rest of the processing. Tokenization errors can occur from:\n",
    "- wrong character set (character encodings not in tokenizer)\n",
    "- wrong language\n",
    "- wrong sentence detection or no sentences at whole\n",
    "- un-expected tokens (chemical formula, RNA/DNA. part numbers, ...)\n",
    "- wrongly dealing with abbreviations\n",
    "- different spelling variations (212)-123.4567 vs 212 123 4567 for NYC phone numbers.\n",
    "- text mutilation by using removal of stop words or stemming\n",
    "- typos, spelling variations and errors\n",
    "\n",
    "4.a How would you identify potential tokenization problems in your NLP project?\n",
    "\n",
    "\n",
    "4.b In what order would you do the detection and pre-processing to deal with the above problems in your NLP project?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ol5sN4fVfdn"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "4a.\n",
    "To evaluate how well the tokenizer handles various problematic structures, it is essential to create and test cases reflecting the specific domain of the corpus. The first step is to identify the domain of the corpus, as its characteristics dictate the types of edge cases to consider: this might be helpful because for example, scientific texts may include chemical formulas, while legal documents might use specialized jargon and abbreviations.\n",
    "Next, I would develop test cases that encompass a variety of scenarios including special characters such as chemical formulas like \"H2O is the chemical formula for water\" and \"C6H12O6 is glucose,\" as well as phone numbers such as \"Call me at (212)-123-4567\" and \"My number is 212.123.4567.\" I would also include common abbreviations like \"The U.S. government is based in Washington\"...\n",
    "Furthermore, I would incorporate known problematic structures such as phrases with multiple elements like \"Artificial Intelligence (AI)\" and \"The Department of Advanced Computing Sciences\"\n",
    "In the end, I would run these edge cases through the tokenizer and evaluate the output for correct tokenization; it is important to check if the tokenizer effectively maintains context.\n",
    "\n",
    "4b.\n",
    "\n",
    "\n",
    "I would do preprocessing with some test cases, then this will be followed by a detection phase and then another preprocessing to fix the potential errors in the previous pre processing phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdS9JFQ5UXkX"
   },
   "source": [
    "# Submission\n",
    "Please share your Colab notebook by clicking File on the top-left corner. Click under Download on Download .ipynb and upload that file to Canvas."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
